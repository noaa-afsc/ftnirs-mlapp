{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Michael.Zakariaie\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pyCompare\n",
    "import matplotlib\n",
    "import keras_tuner as kt\n",
    "import h5py\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, Normalizer, RobustScaler, MaxAbsScaler\n",
    "from yellowbrick.regressor import PredictionError, ResidualsPlot\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from math import sqrt\n",
    "from keras_tuner.tuners import BayesianOptimization, Hyperband\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import LeakyReLU, Input, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, concatenate\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from scipy.ndimage import uniform_filter1d, gaussian_filter1d, median_filter\n",
    "import pywt\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "sns.set(rc={'figure.figsize': (10, 6)})\n",
    "sns.set(style=\"whitegrid\", font_scale=2)\n",
    "\n",
    "def read_and_clean_data(data_source, drop_outliers=True):\n",
    "    if isinstance(data_source, str):\n",
    "        data = pd.read_csv(data_source)\n",
    "    elif isinstance(data_source, StringIO):\n",
    "        data = pd.read_csv(data_source)\n",
    "    else:\n",
    "        raise ValueError(\"data_source must be a file path (str) or a StringIO object.\")\n",
    "    \n",
    "    if drop_outliers:\n",
    "        data = data[data['sample'] != 'outlier']\n",
    "    \n",
    "    data.rename(columns={\"final_age\": \"Age\"}, inplace=True)\n",
    "    \n",
    "    if data.isnull().values.any():\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "\n",
    "    print(data.head())\n",
    "    print(list(data.columns[0:10]))  # ['file_name', 'sample', 'Age', 'latitude', 'length', 'gear_depth', 'gear_temp', 'wn11476.85064', 'wn11468.60577', 'wn11460.36091']\n",
    "    return data\n",
    "\n",
    "# Savitzky-Golay filter function\n",
    "def savgol_filter_func(data, window_length=17, polyorder=2, deriv=1):\n",
    "    return savgol_filter(data, window_length=window_length, polyorder=polyorder, deriv=deriv)\n",
    "\n",
    "# Moving Average filter function\n",
    "def moving_average_filter(data, size=5):\n",
    "    return uniform_filter1d(data, size=size, axis=1)\n",
    "\n",
    "# Gaussian filter function\n",
    "def gaussian_filter_func(data, sigma=2):\n",
    "    return gaussian_filter1d(data, sigma=sigma, axis=1)\n",
    "\n",
    "# Median filter function\n",
    "def median_filter_func(data, size=5):\n",
    "    return median_filter(data, size=(1, size))\n",
    "\n",
    "# Wavelet filter function\n",
    "def wavelet_filter_func(data, wavelet='db1', level=1):\n",
    "    def apply_wavelet(signal):\n",
    "        coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "        coeffs[1:] = [pywt.threshold(i, value=0.5 * max(i)) for i in coeffs[1:]]\n",
    "        return pywt.waverec(coeffs, wavelet)\n",
    "    \n",
    "    return np.apply_along_axis(apply_wavelet, axis=1, arr=data)\n",
    "\n",
    "# Fourier filter function\n",
    "def fourier_filter_func(data, threshold=0.1):\n",
    "    def apply_fft(signal):\n",
    "        fft_data = np.fft.fft(signal)\n",
    "        frequencies = np.fft.fftfreq(len(signal))\n",
    "        fft_data[np.abs(frequencies) > threshold] = 0\n",
    "        return np.fft.ifft(fft_data).real\n",
    "    \n",
    "    return np.apply_along_axis(apply_fft, axis=1, arr=data)\n",
    "\n",
    "# PCA filter function\n",
    "def pca_filter_func(data, n_components=5):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    transformed = pca.fit_transform(data)\n",
    "    return pca.inverse_transform(transformed)\n",
    "\n",
    "# Main preprocessing function\n",
    "def preprocess_spectra(data, filter_type='savgol'):\n",
    "    filter_functions = {\n",
    "        'savgol': savgol_filter_func,\n",
    "        'moving_average': moving_average_filter,\n",
    "        'gaussian': gaussian_filter_func,\n",
    "        'median': median_filter_func,\n",
    "        'wavelet': wavelet_filter_func,\n",
    "        'fourier': fourier_filter_func,\n",
    "        'pca': pca_filter_func\n",
    "    }\n",
    "    \n",
    "    filter_func = filter_functions.get(filter_type, savgol_filter_func)\n",
    "    \n",
    "    data.loc[data['sample'] == 'training', data.columns[4:]] = filter_func(data.loc[data['sample'] == 'training', data.columns[4:]].values)\n",
    "    data.loc[data['sample'] == 'test', data.columns[4:]] = filter_func(data.loc[data['sample'] == 'test', data.columns[4:]].values)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def apply_normalization(data, columns):\n",
    "    normalizer = Normalizer()\n",
    "    data[columns] = normalizer.fit_transform(data[columns])\n",
    "    return data\n",
    "\n",
    "def apply_robust_scaling(data, y_col, feature_columns):\n",
    "    scaler_y = RobustScaler()\n",
    "    data[y_col] = scaler_y.fit_transform(data[[y_col]])\n",
    "    data[feature_columns] = data[feature_columns].apply(lambda col: RobustScaler().fit_transform(col.values.reshape(-1, 1)))\n",
    "    return data, scaler_y\n",
    "\n",
    "def apply_minmax_scaling(data, y_col, feature_columns):\n",
    "    scaler_y = MinMaxScaler()\n",
    "    data[y_col] = scaler_y.fit_transform(data[[y_col]])\n",
    "    data[feature_columns] = data[feature_columns].apply(lambda col: MinMaxScaler().fit_transform(col.values.reshape(-1, 1)))\n",
    "    return data, scaler_y\n",
    "\n",
    "def apply_maxabs_scaling(data, y_col, feature_columns):\n",
    "    scaler_y = MaxAbsScaler()\n",
    "    data[y_col] = scaler_y.fit_transform(data[[y_col]])\n",
    "    data[feature_columns] = data[feature_columns].apply(lambda col: MaxAbsScaler().fit_transform(col.values.reshape(-1, 1)))\n",
    "    return data, scaler_y\n",
    "\n",
    "def apply_scaling(data, scaling_method='standard', y_col='Age'):\n",
    "    scalers = {\n",
    "        'standard': StandardScaler(),\n",
    "        'minmax': MinMaxScaler(),\n",
    "        'maxabs': MaxAbsScaler(),\n",
    "        'robust': RobustScaler(),\n",
    "        'normalize': Normalizer()  \n",
    "    }\n",
    "    \n",
    "    if scaling_method not in scalers:\n",
    "        raise ValueError(f\"Unsupported scaling method: {scaling_method}\")\n",
    "    \n",
    "    feature_columns = data.columns.difference(['sample', 'file_name', y_col])\n",
    "\n",
    "    if scaling_method == 'normalize':\n",
    "        data = apply_normalization(data, feature_columns)\n",
    "        return data, None  \n",
    "    \n",
    "    scaler_y = scalers[scaling_method]\n",
    "    data[y_col] = scaler_y.fit_transform(data[[y_col]])\n",
    "    scaler_x = scalers[scaling_method]\n",
    "    data[feature_columns] = scaler_x.fit_transform(data[feature_columns])\n",
    "    \n",
    "    return data, scaler_y\n",
    "\n",
    "def build_model(hp, input_dim_A, input_dim_B):\n",
    "    input_A = Input(shape=(input_dim_A,))\n",
    "    x = input_A\n",
    "\n",
    "    input_B = Input(shape=(input_dim_B, 1))\n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    num_conv_layers = hp.Int('num_conv_layers', 1, 4, default=1)\n",
    "    kernel_size = hp.Int('kernel_size', 51, 201, step=10, default=101)\n",
    "    stride_size = hp.Int('stride_size', 26, 101, step=5, default=51)\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.1, 0.5, step=0.05, default=0.1)\n",
    "    use_max_pooling = hp.Boolean('use_max_pooling', default=False)\n",
    "    num_filters = hp.Int('num_filters', 50, 100, step=10, default=50)\n",
    "\n",
    "    y = input_B\n",
    "    for i in range(num_conv_layers):\n",
    "        y = Conv1D(\n",
    "            filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=stride_size,\n",
    "            activation='relu',\n",
    "            padding='same')(y)\n",
    "        \n",
    "        # Ensure the input size is appropriate for max pooling\n",
    "        if use_max_pooling and y.shape[1] > 1:\n",
    "            y = MaxPooling1D(pool_size=2)(y)\n",
    "        \n",
    "        y = Dropout(dropout_rate)(y)\n",
    "\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(4, activation=\"relu\", name='output_B')(y)\n",
    "\n",
    "    con = concatenate([x, y])\n",
    "\n",
    "    z = Dense(\n",
    "        hp.Int('dense', 4, 640, step=32, default=256),\n",
    "        activation='relu')(con)\n",
    "    z = Dropout(hp.Float('dropout-2', 0.0, 0.5, step=0.05, default=0.0))(z)\n",
    "\n",
    "    output = Dense(1, activation=\"linear\")(z)\n",
    "    model = Model(inputs=[input_A, input_B], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "def train_and_optimize_model(tuner, data, nb_epoch, batch_size):\n",
    "    outputFilePath = 'Estimator'\n",
    "    checkpointer = ModelCheckpoint(filepath=outputFilePath, verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=7, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    X_train_biological_data = data.loc[data['sample'] == 'training', data.columns[4:8]]\n",
    "    X_train_wavenumbers = data.loc[data['sample'] == 'training', data.columns[8:]]\n",
    "    y_train = data.loc[data['sample'] == 'training', 'Age']\n",
    "\n",
    "    tuner.search([X_train_biological_data, X_train_wavenumbers], y_train,\n",
    "                 epochs=nb_epoch,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 validation_split=0.25,\n",
    "                 verbose=1,\n",
    "                 callbacks=[earlystop])\n",
    "\n",
    "    model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "    return model, best_hp\n",
    "\n",
    "def final_training_pass(model, data, nb_epoch, batch_size):\n",
    "    outputFilePath = 'Estimator'\n",
    "    checkpointer = ModelCheckpoint(filepath=outputFilePath, verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=100, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    X_train_biological_data = data.loc[data['sample'] == 'training', data.columns[4:8]]\n",
    "    X_train_wavenumbers = data.loc[data['sample'] == 'training', data.columns[8:]]\n",
    "    y_train = data.loc[data['sample'] == 'training', 'Age']\n",
    "\n",
    "    history = model.fit([X_train_biological_data, X_train_wavenumbers], y_train,\n",
    "                        epochs=nb_epoch,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_split=0.25,\n",
    "                        verbose=1,\n",
    "                        callbacks=[checkpointer, earlystop]).history\n",
    "    \n",
    "    return history\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    X_test_biological_data = data.loc[data['sample'] == 'test', data.columns[4:8]]\n",
    "    X_test_wavenumbers = data.loc[data['sample'] == 'test', data.columns[8:]]\n",
    "    y_test = data.loc[data['sample'] == 'test', 'Age']\n",
    "\n",
    "    evaluation = model.evaluate([X_test_biological_data, X_test_wavenumbers], y_test)\n",
    "    preds = model.predict([X_test_biological_data, X_test_wavenumbers])\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    return evaluation, preds, r2\n",
    "\n",
    "def plot_predictions(y_test, preds):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, preds)\n",
    "    plt.xlabel('True')\n",
    "    plt.ylabel('Predicted')\n",
    "    lims = [-2.5, 5]\n",
    "    plt.xlim(lims)\n",
    "    plt.ylim(lims)\n",
    "    plt.plot(lims, lims)\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediction_error(preds, y_test):\n",
    "    preds = np.array(preds).flatten()\n",
    "    y_test = y_test.to_numpy().flatten()\n",
    "    error = preds - y_test\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.hist(error, bins=20)\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_training_set(model, data, scaler_y):\n",
    "    X_train_biological_data = data.loc[data['sample'] == 'training', data.columns[4:8]]\n",
    "    X_train_wavenumbers = data.loc[data['sample'] == 'training', data.columns[8:]]\n",
    "    y_train = data.loc[data['sample'] == 'training', 'Age']\n",
    "    f_train = data.loc[data['sample'] == 'training', 'file_name']\n",
    "\n",
    "    y_train = np.array(y_train).reshape(-1, 1)\n",
    "    preds_t = model.predict([X_train_biological_data, X_train_wavenumbers])\n",
    "    \n",
    "    preds_t = preds_t.reshape(-1, 1)\n",
    "    y_train_reshaped = y_train.reshape(-1, 1)\n",
    "    \n",
    "    y_pr_transformed = scaler_y.inverse_transform(preds_t)\n",
    "    y_tr_transformed = scaler_y.inverse_transform(y_train_reshaped)\n",
    "\n",
    "    r_squared_tr = r2_score(y_tr_transformed, y_pr_transformed)\n",
    "    rmse_tr = sqrt(mean_squared_error(y_tr_transformed, y_pr_transformed))\n",
    "\n",
    "    y_tr_df = pd.DataFrame(y_tr_transformed, columns=['train'])\n",
    "    y_tr_df['pred'] = y_pr_transformed\n",
    "    y_tr_df['file'] = f_train.reset_index(drop=True)\n",
    "\n",
    "    y_tr_df.to_csv('./Output/Data/train_predictions.csv', index=False)\n",
    "\n",
    "    return r_squared_tr, rmse_tr, y_tr_df\n",
    "\n",
    "def plot_training_set(y_tr_transformed, y_pr_transformed):\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set(style=\"ticks\")\n",
    "    sns.set_context(\"poster\")\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(12, 12))\n",
    "    p = sns.regplot(x=y_tr_transformed, y=y_pr_transformed, ci=None,\n",
    "                    scatter_kws={\"edgecolor\": 'b', 'linewidths': 2, \"alpha\": 0.5, \"s\": 150},\n",
    "                    line_kws={\"alpha\": 0.5, \"lw\": 4})\n",
    "    ax.plot([y_tr_transformed.min(), y_tr_transformed.max()], [y_tr_transformed.min(), y_tr_transformed.max()], 'k--', lw=2)\n",
    "\n",
    "    p.set(xlim=(-1, 24))\n",
    "    p.set(ylim=(-1, 24))\n",
    "    sns.despine()\n",
    "    plt.title('Training Set', fontsize=25)\n",
    "    plt.xlabel('Traditional Age (years)')\n",
    "    plt.ylabel('FT-NIR Age (years)')\n",
    "    plt.savefig('./Output/Figures/TrainingSet.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_test_set(y_test_transformed, y_pred_transformed):\n",
    "    f, ax = plt.subplots(figsize=(12, 12))\n",
    "    p = sns.regplot(x=y_test_transformed, y=y_pred_transformed, ci=None,\n",
    "                    scatter_kws={\"edgecolor\": 'b', 'linewidths': 2, \"alpha\": 0.5, \"s\": 150},\n",
    "                    line_kws={\"alpha\": 0.5, \"lw\": 4})\n",
    "    ax.plot([y_test_transformed.min(), y_test_transformed.max()], [y_test_transformed.min(), y_test_transformed.max()], 'k--', lw=2)\n",
    "\n",
    "    p.set(xlim=(-1, 24))\n",
    "    p.set(ylim=(-1, 24))\n",
    "    sns.despine()\n",
    "    plt.title('Test Set', fontsize=25)\n",
    "    plt.xlabel('Traditional Age (years)')\n",
    "    plt.ylabel('FT-NIR Age (years)')\n",
    "    plt.savefig('./Output/Figures/TestSet.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_bland_altman(y_test_transformed, y_pred_transformed):\n",
    "    pyCompare.blandAltman(y_test_transformed.flatten(), y_pred_transformed.flatten(),\n",
    "                          limitOfAgreement=1.96, confidenceInterval=95,\n",
    "                          confidenceIntervalMethod='approximate',\n",
    "                          detrend=None, percentage=False,\n",
    "                          title='Bland-Altman Plot\\n',\n",
    "                          savePath='./Output/Figures/BlandAltman.png')\n",
    "\n",
    "def build_model_manual(input_dim_A, input_dim_B, num_conv_layers, kernel_size, stride_size, dropout_rate, use_max_pooling, num_filters, dense_units, dropout_rate_2):\n",
    "    input_A = Input(shape=(input_dim_A,))\n",
    "    x = input_A\n",
    "\n",
    "    input_B = Input(shape=(input_dim_B, 1))\n",
    "    y = input_B\n",
    "    for i in range(num_conv_layers):\n",
    "        y = Conv1D(\n",
    "            filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=stride_size,\n",
    "            activation='relu',\n",
    "            padding='same')(y)\n",
    "        \n",
    "        if use_max_pooling and y.shape[1] > 1:\n",
    "            y = MaxPooling1D(pool_size=2)(y)\n",
    "        \n",
    "        y = Dropout(dropout_rate)(y)\n",
    "\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(4, activation=\"relu\", name='output_B')(y)\n",
    "\n",
    "    con = concatenate([x, y])\n",
    "\n",
    "    z = Dense(dense_units, activation='relu')(con)\n",
    "    z = Dropout(dropout_rate_2)(z)\n",
    "\n",
    "    output = Dense(1, activation=\"linear\")(z)\n",
    "    model = Model(inputs=[input_A, input_B], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "def InferenceMode(model_or_path, data, row_number, scaler_y=None, scaler_x=None):\n",
    "    if isinstance(model_or_path, str):\n",
    "        # Load model from disk\n",
    "        model = load_model(model_or_path)\n",
    "    else:\n",
    "        # Use the provided model object\n",
    "        model = model_or_path\n",
    "    \n",
    "    # Extract the specific row for inference\n",
    "    sample_data = data.iloc[row_number]\n",
    "    \n",
    "    # Extract biological and wavenumber data separately\n",
    "    biological_data = sample_data[data.columns[4:8]].values.reshape(1, -1)\n",
    "    wavenumber_data = sample_data[data.columns[8:]].values.reshape(1, -1, 1)\n",
    "    \n",
    "    # Apply the same scaling used during training\n",
    "    if scaler_x:\n",
    "        biological_data = scaler_x.transform(biological_data)\n",
    "        wavenumber_data = scaler_x.transform(wavenumber_data.reshape(1, -1)).reshape(1, -1, 1)\n",
    "    \n",
    "    # Run inference\n",
    "    prediction = model.predict([biological_data, wavenumber_data])\n",
    "    \n",
    "    # Inverse transform the prediction to original scale\n",
    "    if scaler_y:\n",
    "        prediction = scaler_y.inverse_transform(prediction)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def TrainingModeWithHyperband(filepath, filter_CHOICE, scaling_CHOICE):\n",
    "    data = read_and_clean_data(filepath)\n",
    "    \n",
    "    data = preprocess_spectra(data, filter_type=filter_CHOICE)\n",
    "    \n",
    "    y_col = 'Age'\n",
    "    scaling_method = scaling_CHOICE  # 'minmax', 'standard', 'maxabs', 'robust', or 'normalize'\n",
    "    data, scaler_y = apply_scaling(data, scaling_method, y_col)\n",
    "\n",
    "    input_dim_A = data.columns[4:8].shape[0]\n",
    "    input_dim_B = data.columns[8:].shape[0]\n",
    "    \n",
    "    def model_builder(hp):\n",
    "        return build_model(hp, input_dim_A, input_dim_B)\n",
    "    \n",
    "    tuner = Hyperband(\n",
    "        model_builder,\n",
    "        objective='val_loss',\n",
    "        max_epochs=1,\n",
    "        directory='Tuners',\n",
    "        project_name='mmcnn',\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(tuner.search_space_summary())\n",
    "    \n",
    "    nb_epoch = 1  # !@!\n",
    "    batch_size = 32\n",
    "    model, best_hp = train_and_optimize_model(tuner, data, nb_epoch, batch_size)\n",
    "    \n",
    "    nb_epoch = 1  # !@!\n",
    "    batch_size = 32\n",
    "    history = final_training_pass(model, data, nb_epoch, batch_size)\n",
    "    \n",
    "    evaluation, preds, r2 = evaluate_model(model, data)\n",
    "    print(f\"Evaluation: {evaluation}, R2: {r2}\")\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    training_outputs = {\n",
    "        'trained_model': model,\n",
    "        'scaler' : scaler_y,\n",
    "        'training_history': history,\n",
    "        'evaluation': evaluation,\n",
    "        'predictions': preds,\n",
    "        'r2_score': r2\n",
    "    }\n",
    "    \n",
    "    # top_5_models = tuner.get_best_models(num_models=5)\n",
    "    \n",
    "    additional_outputs = {\n",
    "        # 'top5models': top_5_models\n",
    "    }\n",
    "    \n",
    "    return training_outputs, additional_outputs\n",
    "\n",
    "def TrainingModeWithoutHyperband(filepath, filter_CHOICE, scaling_CHOICE, model_parameters):\n",
    "    if len(model_parameters) != 8:\n",
    "        raise ValueError(\"model_parameters must be a list of 8 values.\")\n",
    "    \n",
    "    num_conv_layers, kernel_size, stride_size, dropout_rate, use_max_pooling, num_filters, dense_units, dropout_rate_2 = model_parameters\n",
    "    \n",
    "    if not all(isinstance(param, (int, float, bool)) for param in model_parameters):\n",
    "        raise ValueError(\"All model parameters must be either int, float, or bool.\")\n",
    "    \n",
    "    data = read_and_clean_data(filepath)\n",
    "    \n",
    "    data = preprocess_spectra(data, filter_type=filter_CHOICE)\n",
    "    \n",
    "    y_col = 'Age'\n",
    "    scaling_method = scaling_CHOICE  # 'minmax', 'standard', 'maxabs', 'robust', or 'normalize'\n",
    "    data, scaler_y = apply_scaling(data, scaling_method, y_col)\n",
    "\n",
    "    input_dim_A = data.columns[4:8].shape[0]\n",
    "    input_dim_B = data.columns[8:].shape[0]\n",
    "\n",
    "    model = build_model_manual(\n",
    "        input_dim_A,\n",
    "        input_dim_B,\n",
    "        num_conv_layers,\n",
    "        kernel_size,\n",
    "        stride_size,\n",
    "        dropout_rate,\n",
    "        use_max_pooling,\n",
    "        num_filters,\n",
    "        dense_units,\n",
    "        dropout_rate_2\n",
    "    )\n",
    "    \n",
    "    nb_epoch = 1  # !@!\n",
    "    batch_size = 32\n",
    "    history = final_training_pass(model, data, nb_epoch, batch_size)\n",
    "    \n",
    "    evaluation, preds, r2 = evaluate_model(model, data)\n",
    "    print(f\"Evaluation: {evaluation}, R2: {r2}\")\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    training_outputs = {\n",
    "        'trained_model': model,\n",
    "        'scaler': scaler_y,\n",
    "        'training_history': history,\n",
    "        'evaluation': evaluation,\n",
    "        'predictions': preds,\n",
    "        'r2_score': r2\n",
    "    }\n",
    "    \n",
    "    additional_outputs = {\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return training_outputs, additional_outputs\n",
    "\n",
    "\n",
    "def saveModel(model, path):\n",
    "    model.save(path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def saveModelWithMetadata(model, path, column_names, description):\n",
    "\n",
    "    # Save the Keras model\n",
    "    model.save(path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "    # Append metadata to the HDF5 file\n",
    "    with h5py.File(path, 'a') as f:\n",
    "        # Create a group for metadata if it doesn't exist\n",
    "        if 'metadata' not in f:\n",
    "            metadata_group = f.create_group('metadata')\n",
    "        else:\n",
    "            metadata_group = f['metadata']\n",
    "\n",
    "        # Add the column names list\n",
    "        if 'column_names' in metadata_group:\n",
    "            del metadata_group['column_names']  # Delete if exists to avoid duplication\n",
    "        metadata_group.create_dataset('column_names', data=column_names)\n",
    "        \n",
    "        # Add the description string\n",
    "        if 'description' in metadata_group:\n",
    "            del metadata_group['description']  # Delete if exists to avoid duplication\n",
    "        metadata_group.create_dataset('description', data=description)\n",
    "        \n",
    "    print(f\"Metadata added to {path}\")\n",
    "\n",
    "def loadModelMetadata(path):\n",
    "\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        metadata_group = f['metadata']\n",
    "        column_names = list(metadata_group['column_names'])\n",
    "        description = metadata_group['description'][()].decode('utf-8')\n",
    "\n",
    "    return column_names, description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sample</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>length</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>sex_MF</th>\n",
       "      <th>sex_immature</th>\n",
       "      <th>variable1</th>\n",
       "      <th>...</th>\n",
       "      <th>wavenumber992</th>\n",
       "      <th>wavenumber993</th>\n",
       "      <th>wavenumber994</th>\n",
       "      <th>wavenumber995</th>\n",
       "      <th>wavenumber996</th>\n",
       "      <th>wavenumber997</th>\n",
       "      <th>wavenumber998</th>\n",
       "      <th>wavenumber999</th>\n",
       "      <th>wavenumber1000</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file_0</td>\n",
       "      <td>sample_0</td>\n",
       "      <td>40</td>\n",
       "      <td>74.293746</td>\n",
       "      <td>67.393900</td>\n",
       "      <td>29.396157</td>\n",
       "      <td>-140.632610</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292993</td>\n",
       "      <td>0.370674</td>\n",
       "      <td>-0.450012</td>\n",
       "      <td>-0.267878</td>\n",
       "      <td>-0.960327</td>\n",
       "      <td>-0.141685</td>\n",
       "      <td>0.064486</td>\n",
       "      <td>-0.018902</td>\n",
       "      <td>0.756517</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file_1</td>\n",
       "      <td>sample_1</td>\n",
       "      <td>71</td>\n",
       "      <td>80.536339</td>\n",
       "      <td>84.156770</td>\n",
       "      <td>-52.476237</td>\n",
       "      <td>-105.564630</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495953</td>\n",
       "      <td>-0.502334</td>\n",
       "      <td>-0.161894</td>\n",
       "      <td>0.539667</td>\n",
       "      <td>0.693358</td>\n",
       "      <td>0.391822</td>\n",
       "      <td>-0.199221</td>\n",
       "      <td>-0.907413</td>\n",
       "      <td>-0.515551</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file_2</td>\n",
       "      <td>sample_2</td>\n",
       "      <td>36</td>\n",
       "      <td>4.440853</td>\n",
       "      <td>11.190370</td>\n",
       "      <td>10.530733</td>\n",
       "      <td>148.616854</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.555201</td>\n",
       "      <td>-0.227633</td>\n",
       "      <td>-0.595870</td>\n",
       "      <td>-0.256127</td>\n",
       "      <td>-0.673387</td>\n",
       "      <td>-0.010108</td>\n",
       "      <td>-0.610757</td>\n",
       "      <td>0.321251</td>\n",
       "      <td>-0.014194</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file_3</td>\n",
       "      <td>sample_3</td>\n",
       "      <td>97</td>\n",
       "      <td>48.577283</td>\n",
       "      <td>82.894957</td>\n",
       "      <td>46.078772</td>\n",
       "      <td>110.813779</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.631553</td>\n",
       "      <td>-0.610297</td>\n",
       "      <td>0.853245</td>\n",
       "      <td>0.873920</td>\n",
       "      <td>-0.036831</td>\n",
       "      <td>0.483452</td>\n",
       "      <td>-0.963729</td>\n",
       "      <td>0.235767</td>\n",
       "      <td>-0.268669</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file_4</td>\n",
       "      <td>sample_4</td>\n",
       "      <td>45</td>\n",
       "      <td>98.577659</td>\n",
       "      <td>48.228708</td>\n",
       "      <td>29.956501</td>\n",
       "      <td>-160.821201</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>-0.472763</td>\n",
       "      <td>-0.599434</td>\n",
       "      <td>-0.423080</td>\n",
       "      <td>-0.726073</td>\n",
       "      <td>-0.888681</td>\n",
       "      <td>0.284311</td>\n",
       "      <td>0.977434</td>\n",
       "      <td>0.190168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>file_95</td>\n",
       "      <td>sample_95</td>\n",
       "      <td>88</td>\n",
       "      <td>66.900108</td>\n",
       "      <td>47.670039</td>\n",
       "      <td>12.918348</td>\n",
       "      <td>-114.916709</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.494411</td>\n",
       "      <td>0.714211</td>\n",
       "      <td>-0.756923</td>\n",
       "      <td>0.876346</td>\n",
       "      <td>0.621658</td>\n",
       "      <td>-0.774376</td>\n",
       "      <td>0.786157</td>\n",
       "      <td>0.772745</td>\n",
       "      <td>0.167485</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>file_96</td>\n",
       "      <td>sample_96</td>\n",
       "      <td>15</td>\n",
       "      <td>64.302037</td>\n",
       "      <td>80.841076</td>\n",
       "      <td>-42.226861</td>\n",
       "      <td>163.146555</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621561</td>\n",
       "      <td>0.857371</td>\n",
       "      <td>0.257363</td>\n",
       "      <td>0.123415</td>\n",
       "      <td>-0.825802</td>\n",
       "      <td>-0.862001</td>\n",
       "      <td>0.920994</td>\n",
       "      <td>-0.824751</td>\n",
       "      <td>0.747975</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>file_97</td>\n",
       "      <td>sample_97</td>\n",
       "      <td>9</td>\n",
       "      <td>91.015599</td>\n",
       "      <td>90.756110</td>\n",
       "      <td>-89.967867</td>\n",
       "      <td>-100.311738</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984336</td>\n",
       "      <td>0.648759</td>\n",
       "      <td>-0.235680</td>\n",
       "      <td>0.103255</td>\n",
       "      <td>-0.361236</td>\n",
       "      <td>0.460311</td>\n",
       "      <td>-0.739865</td>\n",
       "      <td>0.786491</td>\n",
       "      <td>0.417692</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>file_98</td>\n",
       "      <td>sample_98</td>\n",
       "      <td>46</td>\n",
       "      <td>29.213936</td>\n",
       "      <td>63.652627</td>\n",
       "      <td>-71.220671</td>\n",
       "      <td>-84.263834</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093052</td>\n",
       "      <td>-0.928540</td>\n",
       "      <td>0.505762</td>\n",
       "      <td>0.358940</td>\n",
       "      <td>0.300087</td>\n",
       "      <td>0.460344</td>\n",
       "      <td>-0.770619</td>\n",
       "      <td>0.612616</td>\n",
       "      <td>0.157162</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>file_99</td>\n",
       "      <td>sample_99</td>\n",
       "      <td>8</td>\n",
       "      <td>81.919729</td>\n",
       "      <td>93.271362</td>\n",
       "      <td>59.540965</td>\n",
       "      <td>148.165356</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416405</td>\n",
       "      <td>-0.421245</td>\n",
       "      <td>0.832059</td>\n",
       "      <td>0.828172</td>\n",
       "      <td>-0.454113</td>\n",
       "      <td>0.771270</td>\n",
       "      <td>-0.668650</td>\n",
       "      <td>0.871207</td>\n",
       "      <td>0.535299</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 1021 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   filename     sample  age     weight     length   latitude   longitude  \\\n",
       "0    file_0   sample_0   40  74.293746  67.393900  29.396157 -140.632610   \n",
       "1    file_1   sample_1   71  80.536339  84.156770 -52.476237 -105.564630   \n",
       "2    file_2   sample_2   36   4.440853  11.190370  10.530733  148.616854   \n",
       "3    file_3   sample_3   97  48.577283  82.894957  46.078772  110.813779   \n",
       "4    file_4   sample_4   45  98.577659  48.228708  29.956501 -160.821201   \n",
       "..      ...        ...  ...        ...        ...        ...         ...   \n",
       "95  file_95  sample_95   88  66.900108  47.670039  12.918348 -114.916709   \n",
       "96  file_96  sample_96   15  64.302037  80.841076 -42.226861  163.146555   \n",
       "97  file_97  sample_97    9  91.015599  90.756110 -89.967867 -100.311738   \n",
       "98  file_98  sample_98   46  29.213936  63.652627 -71.220671  -84.263834   \n",
       "99  file_99  sample_99    8  81.919729  93.271362  59.540965  148.165356   \n",
       "\n",
       "    sex_MF  sex_immature  variable1  ...  wavenumber992  wavenumber993  \\\n",
       "0        0             0          4  ...       0.292993       0.370674   \n",
       "1        0             0          7  ...       0.495953      -0.502334   \n",
       "2        0             0          1  ...      -0.555201      -0.227633   \n",
       "3        0             1          5  ...      -0.631553      -0.610297   \n",
       "4        0             0          0  ...       0.002465      -0.472763   \n",
       "..     ...           ...        ...  ...            ...            ...   \n",
       "95       0             1          1  ...      -0.494411       0.714211   \n",
       "96       1             0         -1  ...       0.621561       0.857371   \n",
       "97       1             0          1  ...       0.984336       0.648759   \n",
       "98       0             1          8  ...      -0.093052      -0.928540   \n",
       "99       1             1          7  ...       0.416405      -0.421245   \n",
       "\n",
       "    wavenumber994  wavenumber995  wavenumber996  wavenumber997  wavenumber998  \\\n",
       "0       -0.450012      -0.267878      -0.960327      -0.141685       0.064486   \n",
       "1       -0.161894       0.539667       0.693358       0.391822      -0.199221   \n",
       "2       -0.595870      -0.256127      -0.673387      -0.010108      -0.610757   \n",
       "3        0.853245       0.873920      -0.036831       0.483452      -0.963729   \n",
       "4       -0.599434      -0.423080      -0.726073      -0.888681       0.284311   \n",
       "..            ...            ...            ...            ...            ...   \n",
       "95      -0.756923       0.876346       0.621658      -0.774376       0.786157   \n",
       "96       0.257363       0.123415      -0.825802      -0.862001       0.920994   \n",
       "97      -0.235680       0.103255      -0.361236       0.460311      -0.739865   \n",
       "98       0.505762       0.358940       0.300087       0.460344      -0.770619   \n",
       "99       0.832059       0.828172      -0.454113       0.771270      -0.668650   \n",
       "\n",
       "    wavenumber999  wavenumber1000  metadata  \n",
       "0       -0.018902        0.756517    4000.0  \n",
       "1       -0.907413       -0.515551    9000.0  \n",
       "2        0.321251       -0.014194       5.0  \n",
       "3        0.235767       -0.268669       NaN  \n",
       "4        0.977434        0.190168       NaN  \n",
       "..            ...             ...       ...  \n",
       "95       0.772745        0.167485       NaN  \n",
       "96      -0.824751        0.747975       NaN  \n",
       "97       0.786491        0.417692       NaN  \n",
       "98       0.612616        0.157162       NaN  \n",
       "99       0.871207        0.535299       NaN  \n",
       "\n",
       "[100 rows x 1021 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataframe():\n",
    "    num_rows = 100\n",
    "\n",
    "    data = {\n",
    "        'filename': [f'file_{i}' for i in range(num_rows)],\n",
    "        'sample': [f'sample_{i}' for i in range(num_rows)],\n",
    "        'age': np.random.randint(1, 100, num_rows),\n",
    "        'weight': np.random.uniform(1.0, 100.0, num_rows),\n",
    "        'length': np.random.uniform(1.0, 100.0, num_rows),\n",
    "        'latitude': np.random.uniform(-90.0, 90.0, num_rows),\n",
    "        'longitude': np.random.uniform(-180.0, 180.0, num_rows),\n",
    "        'sex_MF': np.random.choice([0, 1], num_rows),\n",
    "        'sex_immature': np.random.choice([0, 1], num_rows),\n",
    "    }\n",
    "\n",
    "    for i in range(1, 12):\n",
    "        data[f'variable{i}'] = np.random.randint(-1, 10, num_rows)\n",
    "\n",
    "    for i in range(1, 1001):\n",
    "        data[f'wavenumber{i}'] = np.random.uniform(-1.0, 1.0, num_rows)\n",
    "\n",
    "    data['metadata'] = [4000, 9000, 5] + [np.nan] * (num_rows - 3)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "sample_data_format = create_dataframe()\n",
    "\n",
    "sample_data_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training With Hyperband\n",
    "training_outputs_hyperband, additional_outputs_hyperband = TrainingModeWithHyperband(\n",
    "    filepath='./Data/AGP_MMCNN_BSsurvey_pollock2014to2018.csv',\n",
    "    filter_CHOICE='savgol',\n",
    "    scaling_CHOICE='minmax'\n",
    ")\n",
    "\n",
    "print(training_outputs_hyperband)\n",
    "print(additional_outputs_hyperband)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Without Hyperband\n",
    "\n",
    "training_outputs_manual, additional_outputs_manual = TrainingModeWithoutHyperband(\n",
    "    filepath='./Data/AGP_MMCNN_BSsurvey_pollock2014to2018.csv',\n",
    "    filter_CHOICE='savgol',\n",
    "    scaling_CHOICE='minmax',\n",
    "    model_parameters=[2, 101, 51, 0.1, False, 50, 256, 0.1]\n",
    ")\n",
    "\n",
    "print(training_outputs_manual)\n",
    "print(additional_outputs_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model\n",
    "\n",
    "\n",
    "model = training_outputs_hyperband['trained_model']\n",
    "\n",
    "saveModel(model, \"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = training_outputs_hyperband['trained_model']\n",
    "\n",
    "saveModel(model, \"my_model.h5\")\n",
    "\n",
    "\n",
    "# prediction = InferenceMode(model, data, row_number=5, scaler_y=scaler_y, scaler_x=scaler_x)\n",
    "# print(f\"Prediction for row {row_number}: {prediction}\")\n",
    "\n",
    "column_names = ['col1', 'col2', 'col3']\n",
    "description = 'This model is trained on data columns col1, col2, col3'\n",
    "saveModelWithMetadata(model, 'my_model_with_metadata.h5', column_names, description)\n",
    "\n",
    "# Example usage:\n",
    "metadata_path = 'my_model_with_metadata.h5'\n",
    "column_names, description = loadModelMetadata(metadata_path)\n",
    "print(f\"Column Names: {column_names}\")\n",
    "print(f\"Description: {description}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
